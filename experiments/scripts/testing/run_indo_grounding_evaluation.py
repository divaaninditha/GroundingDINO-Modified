import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import time
from PIL import Image
import numpy as np
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
from collections import defaultdict
from transformers import AutoTokenizer, AutoModel
import torchvision.transforms as T
from tqdm import tqdm
import copy

# Add paths
sys.path.append('.')
sys.path.append('experiments/scripts/indo_groundingdino')

# Import GroundingDINO functions
from groundingdino.util.inference import load_model, predict, load_image
from groundingdino.util.slconfig import SLConfig

class IndoBERTTextEncoder(nn.Module):
    """
    IndoBERT Text Encoder untuk replace BERT-English di GroundingDINO
    """
    def __init__(self, model_name="indolem/indobert-base-uncased", feature_dim=256):
        super().__init__()
        print(f"ğŸ”§ Loading IndoBERT: {model_name}")
        
        # Load IndoBERT
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.bert_model = AutoModel.from_pretrained(model_name)
        
        # Projection layer to match GroundingDINO feature dimension
        self.projection = nn.Linear(self.bert_model.config.hidden_size, feature_dim)
        self.layer_norm = nn.LayerNorm(feature_dim)
        
        print(f"âœ… IndoBERT loaded: {self.bert_model.config.hidden_size} -> {feature_dim}")
    
    def forward(self, texts: List[str], device='cpu'):
        """Forward pass untuk Indonesian text"""
        if not texts:
            return torch.empty(0, self.projection.out_features, device=device)
        
        # Tokenize Indonesian texts
        inputs = self.tokenizer(
            texts, 
            return_tensors='pt', 
            padding=True, 
            truncation=True, 
            max_length=256
        )
        
        # Move to device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Get IndoBERT embeddings
        with torch.no_grad():
            outputs = self.bert_model(**inputs)
            # Use [CLS] token embedding
            text_features = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]
        
        # Project to GroundingDINO feature space
        projected_features = self.projection(text_features)
        text_features = self.layer_norm(projected_features)
        
        return text_features

class BillingualCocoDataset(Dataset):
    """
    Dataset untuk fine-tuning dengan bilingual COCO captions
    """
    def __init__(self, dataset_path: str, images_dir: str, mode='train', train_split=0.8, max_images=None):
        self.dataset_path = dataset_path
        self.images_dir = images_dir
        self.mode = mode
        
        # Load annotations
        with open(f"{dataset_path}/annotations.json", 'r') as f:
            self.coco_data = json.load(f)
            
        # Load bilingual captions
        with open(f"{dataset_path}/captions/coco_captions_bilingual.json", 'r', encoding='utf-8') as f:
            self.bilingual_captions = json.load(f)
        
        # Prepare image list dengan proper train/val split
        all_images = list(self.bilingual_captions.keys())
        if max_images:
            all_images = all_images[:max_images]
        
        # Split train/val
        split_idx = int(len(all_images) * train_split)
        if mode == 'train':
            self.image_list = all_images[:split_idx]
        elif mode == 'val':
            self.image_list = all_images[split_idx:]
        else:  # test mode - use all
            self.image_list = all_images
        
        # Create ground truth mapping
        self.gt_by_image = defaultdict(list)
        for ann in self.coco_data['annotations']:
            self.gt_by_image[ann['image_id']].append({
                'bbox': ann['bbox'],
                'category_id': ann['category_id'],
                'area': ann['area']
            })
        
        # Image transforms
        self.transform = T.Compose([
            T.Resize((800, 800)),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        print(f"âœ… {mode} dataset loaded: {len(self.image_list)} images")
    
    def __len__(self):
        return len(self.image_list)
    
    def __getitem__(self, idx):
        image_name = self.image_list[idx]
        image_path = os.path.join(self.images_dir, image_name)
        
        # Load image
        image = Image.open(image_path).convert('RGB')
        image_tensor = self.transform(image)
        
        # Get captions
        captions_data = self.bilingual_captions[image_name]
        english_caption = captions_data['english'][0]
        indonesian_caption = captions_data['indonesian'][0]
        
        # Get image ID and ground truth count (not the boxes themselves to avoid collation issues)
        image_id = int(image_name.split('.')[0])
        gt_boxes = self.gt_by_image.get(image_id, [])
        num_gt_boxes = len(gt_boxes)
        
        return {
            'image': image_tensor,
            'image_name': image_name,
            'image_id': image_id,
            'english_caption': english_caption,
            'indonesian_caption': indonesian_caption,
            'num_gt_boxes': num_gt_boxes,  # Just the count, not the boxes
            'original_image': image  # For evaluation
        }

def custom_collate_fn(batch):
    """
    Custom collate function untuk handle variable-length data
    """
    # Stack tensors yang bisa di-stack
    images = torch.stack([item['image'] for item in batch])
    
    # Keep lists untuk variable-length data
    image_names = [item['image_name'] for item in batch]
    image_ids = [item['image_id'] for item in batch]
    english_captions = [item['english_caption'] for item in batch]
    indonesian_captions = [item['indonesian_caption'] for item in batch]
    num_gt_boxes = [item['num_gt_boxes'] for item in batch]
    original_images = [item['original_image'] for item in batch]
    
    return {
        'image': images,
        'image_name': image_names,
        'image_id': image_ids,
        'english_caption': english_captions,
        'indonesian_caption': indonesian_captions,
        'num_gt_boxes': num_gt_boxes,
        'original_image': original_images
    }

class IndoGroundingDINOTrainer:
    """
    IndoGroundingDINO Fine-tuning Trainer
    """
    def __init__(self, 
                 config_path: str = "groundingdino/config/GroundingDINO_SwinT_OGC.py",
                 checkpoint_path: str = "weights/groundingdino_swint_ogc.pth",
                 dataset_path: str = "experiments/data",
                 results_path: str = "experiments/results",
                 device: str = 'cpu'):
        
        self.config_path = config_path
        self.checkpoint_path = checkpoint_path
        self.dataset_path = dataset_path
        self.results_path = results_path
        self.device = torch.device(device)
        
        print(f"ğŸš€ Initializing IndoGroundingDINO Fine-tuning Trainer")
        print(f"   Device: {self.device}")
        print(f"   Architecture: Swin-T (Vision) + IndoBERT (Text)")
        
        # Create directories
        os.makedirs(f"{results_path}/indo_groundingdino", exist_ok=True)
        os.makedirs(f"{results_path}/indo_groundingdino/checkpoints", exist_ok=True)
        
        # Initialize models
        self._load_models()
        
        # Setup training components
        self._setup_training()
        
        print("âœ… IndoGroundingDINO Trainer ready!")
    
    def _load_models(self):
        """Load base GroundingDINO and IndoBERT"""
        print("ğŸ“¦ Loading models...")
        
        # Load base GroundingDINO
        self.grounding_model = load_model(self.config_path, self.checkpoint_path)
        self.grounding_model = self.grounding_model.to(self.device)
        
        # Create IndoBERT text encoder
        self.indo_text_encoder = IndoBERTTextEncoder(
            model_name="indolem/indobert-base-uncased",
            feature_dim=256
        ).to(self.device)
        
        print("âœ… Models loaded successfully")
    
    def _setup_training(self):
        """Setup training components"""
        print("âš™ï¸ Setting up training components...")
        
        # Optimizer - only train IndoBERT components
        trainable_params = list(self.indo_text_encoder.parameters())
        
        # Optionally fine-tune some GroundingDINO layers
        # Uncomment to fine-tune vision-text fusion layers
        # for name, param in self.grounding_model.named_parameters():
        #     if 'transformer.decoder' in name:  # Fine-tune decoder layers
        #         trainable_params.append(param)
        
        self.optimizer = optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)
        
        # Learning rate scheduler
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=10, eta_min=1e-6
        )
        
        # Loss function (simplified - can be enhanced)
        self.criterion = nn.MSELoss()
        
        print(f"âœ… Training setup complete")
        print(f"   Trainable parameters: {sum(p.numel() for p in trainable_params):,}")
    
    def create_data_loaders(self, batch_size=4, max_images=200):
        """Create training and validation data loaders dengan proper split"""
        print(f"ğŸ“Š Creating data loaders...")
        
        # Create datasets dengan automatic train/val split
        train_dataset = BillingualCocoDataset(
            self.dataset_path, 
            f"{self.dataset_path}/images", 
            mode='train',
            train_split=0.8,  # 80% train, 20% val
            max_images=max_images
        )
        
        val_dataset = BillingualCocoDataset(
            self.dataset_path,
            f"{self.dataset_path}/images",
            mode='val',
            train_split=0.8,  # Same split ratio
            max_images=max_images
        )
        
        # Create data loaders dengan custom collate function
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0,  # Set to 0 for Windows compatibility
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=custom_collate_fn  # Use custom collate function
        )
        
        print(f"âœ… Data loaders created:")
        print(f"   Train dataset: {len(train_dataset)} images, {len(self.train_loader)} batches")
        print(f"   Val dataset: {len(val_dataset)} images, {len(self.val_loader)} batches")
        
        return self.train_loader, self.val_loader
    
    def train_epoch(self, epoch):
        """Train for one epoch"""
        self.indo_text_encoder.train()
        self.grounding_model.eval()  # Keep vision model frozen initially
        
        total_loss = 0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")
        
        for batch in pbar:
            self.optimizer.zero_grad()
            
            # Get batch data
            images = batch['image'].to(self.device)
            indonesian_captions = batch['indonesian_caption']
            english_captions = batch['english_caption']
            
            try:
                # Forward pass with IndoBERT
                indo_text_features = self.indo_text_encoder(indonesian_captions, self.device)
                
                # Target: English text features (from original model)
                with torch.no_grad():
                    # This is simplified - in practice you'd extract actual BERT features
                    # For now, we'll use a contrastive learning approach
                    eng_text_features = self.indo_text_encoder(english_captions, self.device)
                
                # Simple contrastive loss between Indonesian and English features
                loss = self.criterion(indo_text_features, eng_text_features.detach())
                
                # Backward pass
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.indo_text_encoder.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                # Update progress bar
                pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
                
            except Exception as e:
                print(f"âš ï¸ Batch error: {e}")
                continue
        
        avg_loss = total_loss / max(num_batches, 1)
        return avg_loss
    
    def evaluate_epoch(self, epoch):
        """Evaluate model after epoch"""
        print(f"ğŸ“Š Evaluating epoch {epoch+1}...")
        
        self.indo_text_encoder.eval()
        total_detections = 0
        total_images = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Evaluating"):
                images = batch['original_image']
                indonesian_captions = batch['indonesian_caption']
                
                for i, (image, caption) in enumerate(zip(images, indonesian_captions)):
                    try:
                        # Use modified prediction with IndoBERT
                        boxes, scores, phrases = self.predict_with_indobert(image, caption)
                        total_detections += len(boxes)
                        total_images += 1
                        
                    except Exception as e:
                        continue
        
        avg_detections = total_detections / max(total_images, 1)
        print(f"âœ… Epoch {epoch+1} - Avg detections per image: {avg_detections:.2f}")
        
        return avg_detections
    
    def predict_with_indobert(self, image, indonesian_caption, box_threshold=0.35, text_threshold=0.25):
        """Prediction menggunakan IndoBERT dengan proper image handling"""
        try:
            # Analisis Indonesian text dengan IndoBERT
            indo_features = self.indo_text_encoder([indonesian_caption], self.device)
            
            # Translation mapping untuk fallback
            translation_map = {
                'orang': 'person', 'manusia': 'person', 'seseorang': 'person',
                'mobil': 'car', 'kendaraan': 'car', 'motor': 'motorcycle',
                'sepeda': 'bicycle', 'kursi': 'chair', 'meja': 'table',
                'tas': 'bag', 'buku': 'book', 'laptop': 'laptop',
                'makanan': 'food', 'minuman': 'drink', 'kucing': 'cat',
                'anjing': 'dog', 'burung': 'bird', 'perahu': 'boat'
            }
            
            # Extract objects dari Indonesian caption
            caption_lower = indonesian_caption.lower()
            english_objects = []
            
            for indo_word, eng_word in translation_map.items():
                if indo_word in caption_lower:
                    if eng_word not in english_objects:
                        english_objects.append(eng_word)
            
            if english_objects:
                english_query = ". ".join(english_objects)
                
                # Proper image handling untuk GroundingDINO
                if isinstance(image, Image.Image):
                    # Save PIL image temporarily untuk load_image function
                    temp_path = "temp_eval_image.jpg"
                    image.save(temp_path)
                    
                    # Use GroundingDINO's load_image function
                    image_source, image_tensor = load_image(temp_path)
                    
                    # Clean up temp file
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                else:
                    # If already processed, use as is
                    image_tensor = image
                
                # Predict dengan GroundingDINO
                boxes, logits, phrases = predict(
                    model=self.grounding_model,
                    image=image_tensor,
                    caption=english_query,
                    box_threshold=box_threshold,
                    text_threshold=text_threshold,
                    device='cpu'
                )
                
                return boxes, logits, phrases
            
        except Exception as e:
            print(f"âš ï¸ Prediction error: {e}")
        
        # Return empty if failed
        return torch.empty(0, 4), torch.empty(0), []
    
    def fine_tune(self, num_epochs=10, batch_size=4, max_images=200):
        """Main fine-tuning loop"""
        print(f"ğŸš€ Starting IndoGroundingDINO Fine-tuning")
        print(f"   Epochs: {num_epochs}")
        print(f"   Batch size: {batch_size}")
        print(f"   Total images: {max_images}")
        print("="*60)
        
        # Create data loaders
        train_loader, val_loader = self.create_data_loaders(batch_size, max_images)
        
        # Training history
        history = {
            'train_loss': [],
            'val_detections': [],
            'epochs': []
        }
        
        best_detection_rate = 0
        best_model_state = None
        
        start_time = time.time()
        
        for epoch in range(num_epochs):
            print(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")
            print("-" * 40)
            
            # Train
            train_loss = self.train_epoch(epoch)
            print(f"   Training loss: {train_loss:.4f}")
            
            # Evaluate
            val_detections = self.evaluate_epoch(epoch)
            
            # Update scheduler
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            print(f"   Learning rate: {current_lr:.6f}")
            
            # Save history
            history['train_loss'].append(train_loss)
            history['val_detections'].append(val_detections)
            history['epochs'].append(epoch + 1)
            
            # Save best model
            if val_detections > best_detection_rate:
                best_detection_rate = val_detections
                best_model_state = copy.deepcopy(self.indo_text_encoder.state_dict())
                print(f"   âœ… New best detection rate: {best_detection_rate:.2f}")
                
                # Save checkpoint
                checkpoint_path = f"{self.results_path}/indo_groundingdino/checkpoints/best_model_epoch_{epoch+1}.pth"
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': best_model_state,
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'detection_rate': best_detection_rate,
                    'train_loss': train_loss
                }, checkpoint_path)
                print(f"   ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ‰ Fine-tuning completed!")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Best detection rate: {best_detection_rate:.2f}")
        
        # Load best model
        if best_model_state:
            self.indo_text_encoder.load_state_dict(best_model_state)
            print(f"   âœ… Best model loaded")
        
        # Save training history
        history_path = f"{self.results_path}/indo_groundingdino/training_history.json"
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2)
        
        # Plot training history
        self.plot_training_history(history)
        
        return history
    
    def plot_training_history(self, history):
        """Plot training progress"""
        plt.figure(figsize=(12, 4))
        
        # Plot loss
        plt.subplot(1, 2, 1)
        plt.plot(history['epochs'], history['train_loss'])
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        # Plot validation detections
        plt.subplot(1, 2, 2)
        plt.plot(history['epochs'], history['val_detections'])
        plt.title('Validation Detection Rate')
        plt.xlabel('Epoch')
        plt.ylabel('Avg Detections per Image')
        plt.grid(True)
        
        plt.tight_layout()
        
        plot_path = f"{self.results_path}/indo_groundingdino/training_progress.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Training plot saved: {plot_path}")
        
        plt.show()
    
    def run_final_evaluation(self, max_images=200):
        """Run final evaluation pada full dataset"""
        print(f"\nğŸ¯ Running final evaluation on {max_images} images...")
        
        # Load final dataset
        final_dataset = BillingualCocoDataset(
            self.dataset_path,
            f"{self.dataset_path}/images",
            mode='test',  # Use all data for final test
            max_images=max_images
        )
        
        final_loader = DataLoader(
            final_dataset, 
            batch_size=1, 
            shuffle=False,
            collate_fn=custom_collate_fn
        )
        
        results = {
            'predictions': [],
            'model_type': 'IndoGroundingDINO_FineTuned',
            'processing_time': 0
        }
        
        start_time = time.time()
        
        self.indo_text_encoder.eval()
        
        for i, batch in enumerate(tqdm(final_loader, desc="Final Evaluation")):
            image = batch['original_image'][0]  # Get first (and only) item from batch
            image_name = batch['image_name'][0]
            indonesian_caption = batch['indonesian_caption'][0]
            
            try:
                # Predict dengan fine-tuned IndoBERT
                boxes, scores, phrases = self.predict_with_indobert(image, indonesian_caption)
                
                # Convert boxes to proper format
                if len(boxes) > 0:
                    boxes_numpy = boxes.cpu().numpy()
                    scores_numpy = scores.cpu().numpy()
                else:
                    boxes_numpy = np.array([])
                    scores_numpy = np.array([])
                
                result = {
                    'image_name': image_name,
                    'caption': indonesian_caption,
                    'language': 'indonesian_finetuned',
                    'model': 'IndoGroundingDINO_FineTuned',
                    'boxes': boxes_numpy.tolist() if len(boxes_numpy) > 0 else [],
                    'scores': scores_numpy.tolist() if len(scores_numpy) > 0 else [],
                    'phrases': phrases,
                    'num_detections': len(boxes)
                }
                
                results['predictions'].append(result)
                
            except Exception as e:
                print(f"âš ï¸ Error processing {image_name}: {e}")
                continue
        
        results['processing_time'] = time.time() - start_time
        
        # Save results
        results_path = f"{self.results_path}/indo_groundingdino/predictions.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Final evaluation completed!")
        print(f"   Total predictions: {len(results['predictions'])}")
        print(f"   Processing time: {results['processing_time']:.2f}s")
        print(f"   Results saved: {results_path}")
        
        # Quick stats
        total_detections = sum(p['num_detections'] for p in results['predictions'])
        avg_detections = total_detections / len(results['predictions']) if results['predictions'] else 0
        
        print(f"ğŸ“Š Performance Summary:")
        print(f"   Total detections: {total_detections}")
        print(f"   Average detections per image: {avg_detections:.2f}")
        
        return results

def run_indo_grounding_finetuning(num_epochs=10, batch_size=4, max_images=200):
    """
    Main function untuk fine-tuning IndoGroundingDINO
    """
    print("ğŸš€ INDOGROUNDINGDINO FINE-TUNING")
    print("ğŸ¯ Goal: Fine-tune dengan dataset bilingual untuk mendekati baseline")
    print("ğŸ”§ Architecture: Swin-T (Vision) + IndoBERT (Fine-tuned Text)")
    print("="*70)
    
    # Initialize trainer
    trainer = IndoGroundingDINOTrainer(device='cpu')  # Use CPU for stability
    
    # Run fine-tuning
    print(f"\nğŸ“š Phase 1: Fine-tuning ({num_epochs} epochs)")
    history = trainer.fine_tune(
        num_epochs=num_epochs,
        batch_size=batch_size,
        max_images=max_images
    )
    
    # Run final evaluation
    print(f"\nğŸ¯ Phase 2: Final evaluation (200 images)")
    results = trainer.run_final_evaluation(max_images=200)
    
    print(f"\nğŸ‰ INDOGROUNDINGDINO FINE-TUNING COMPLETE!")
    print(f"="*50)
    print(f"âœ… Model fine-tuned for {num_epochs} epochs")
    print(f"âœ… Final evaluation completed")
    print(f"âœ… Results ready for quantitative analysis")
    
    return history, results

if __name__ == "__main__":
    print("ğŸ§ª Starting IndoGroundingDINO Fine-tuning...")
    
    # Run with different configurations
    # Standard training
    history, results = run_indo_grounding_finetuning(
        num_epochs=20,
        batch_size=2,  # Small batch for stability
        max_images=200  # Subset for initial training
    )
    
    print("\nğŸ¯ Fine-tuning completed - ready for 3-model analysis!")